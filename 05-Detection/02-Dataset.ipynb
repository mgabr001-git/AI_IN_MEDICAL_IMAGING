{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "white-chocolate",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will create a custom DataSet which will load and return an X-Ray image together with the location of the heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-limit",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "\n",
    "* Path for easy path handling\n",
    "* torch for dataset creation\n",
    "* numpy for loading the images\n",
    "* pandas for loading the csv file containing the labels\n",
    "* imgaug to set a random seed for augmentations\n",
    "* BoundingBox from imgaug to automatically handle the coordinates when augmenting the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accredited-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import imgaug\n",
    "# from imgaug.augmentables.bbs import BoundingBox\n",
    "\n",
    "# import torchvision\n",
    "\n",
    "from torchvision.tv_tensors import BoundingBoxes\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-awareness",
   "metadata": {},
   "source": [
    "## DataSet Creation\n",
    "Now we define the torch dataset!\n",
    "We need to define a __ len __ function which returns the length of the dataset and a __ getitem __ function which returns the image and corresponding bounding box.\n",
    "\n",
    "Additionally we apply data augmentation and normalization.\n",
    "\n",
    "**Important**: Augment bounding box together with image!<br />\n",
    "You can use *BoundingBox(x1, y1, x2, y2) for that*.<br />\n",
    "Next you call *self.augment(image=img, bounding_boxes=bb)* which returns the augmented image and bounding boxes<br />\n",
    "Finally you extract the coordinates from the augmented bbox coordinates. Note that it is a 2D array.\n",
    "```python\n",
    "bb = BoundingBox(x1=bbox[0], y1=bbox[1], x2=bbox[2], y2=bbox[3])\n",
    "img, aug_bbox  = self.augment(image=img, bounding_boxes=bb)\n",
    "bbox = aug_bbox[0][0], aug_bbox[0][1], aug_bbox[1][0], aug_bbox[1][1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0e7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.49\n",
    "std_mg = 0.082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "universal-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path_to_labels_csv, patients, root_path, augs):\n",
    "        \n",
    "        self.labels = pd.read_csv(path_to_labels_csv)\n",
    "        \n",
    "        self.patients = np.load(patients)\n",
    "        self.root_path = Path(root_path)\n",
    "        self.augment = augs\n",
    "        \n",
    "    def  __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.patients)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns an image paired with bbox around the heart\n",
    "        \"\"\"\n",
    "        patient = self.patients[idx]\n",
    "        # Get data according to index\n",
    "        data = self.labels[self.labels[\"name\"]==patient]\n",
    "        \n",
    "        # Get entries of given patient\n",
    "        # Extract coordinates\n",
    "        \n",
    "        x_min = data[\"x0\"].item()\n",
    "        y_min = data[\"y0\"].item()\n",
    "        x_max = x_min + data[\"w\"].item()  # get xmax from width\n",
    "        y_max = y_min + data[\"h\"].item()  # get ymax from height\n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "        # Load file and convert to float32\n",
    "        file_path = self.root_path/patient  # Create the path to the file\n",
    "        img = np.load(f\"{file_path}.npy\").astype(np.float32)\n",
    "       \n",
    "        # img = torch.tensor(img)\n",
    "\n",
    "        \n",
    "        # Apply imgaug augmentations to image and bounding box\n",
    "        if self.augment:\n",
    "            \n",
    "            bb = BoundingBoxes(\n",
    "            torch.tensor([bbox]),\n",
    "            format=\"XYXY\",\n",
    "            canvas_size=(224, 224)\n",
    "            )\n",
    "\n",
    "\n",
    "            ###################IMPORTANT###################\n",
    "            # Fix for https://discuss.pytorch.org/t/dataloader-workers-generate-the-same-random-augmentations/28830/2\n",
    "            # https://github.com/pytorch/pytorch/issues/5059\n",
    "            # random_seed = torch.randint(0, 1000000, (1,)).item()\n",
    "            # imgaug.seed(random_seed) \n",
    "            #####################################################\n",
    "\n",
    "            # random_seed = torch.randint(0, 1000000, (1,)).item()\n",
    "            # torch.manual_seed(random_seed) \n",
    "\n",
    "            img, aug_bbox  = self.augment(img, bb)\n",
    "            bbox = aug_bbox[0][0], aug_bbox[0][1], aug_bbox[0][2], aug_bbox[0][3]\n",
    "        # Normalize the image according to the values computed in Preprocessing\n",
    "\n",
    "        img = (img - mu) / std_mg\n",
    "\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = torch.tensor(img)\n",
    "        bbox  = torch.tensor(bbox)\n",
    "        return img, bbox\n",
    "        # return np.array(img), np.array(bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-incident",
   "metadata": {},
   "source": [
    "## Validate functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strong-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First create the augmentation object\n",
    "\n",
    "# seq = iaa.Sequential([\n",
    "#             iaa.GammaContrast(),\n",
    "#             iaa.Affine(\n",
    "#                 scale=(0.8, 1.2),\n",
    "#                 rotate=(-10, 10),\n",
    "#                 translate_px=(-10, 10)\n",
    "#             )\n",
    "#         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ceaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "                                    v2.ToImage(),  # Convert numpy array to tensor\n",
    "                                    v2.Normalize(mean=[mu], std=[std_mg]), # (0.49, 0.248),  # Use mean and std from preprocessing notebook\n",
    "                                    v2.RandomAutocontrast(),\n",
    "                                    v2.RandomAffine(degrees=(-10, 10), translate=(0, 0.05), scale=(0.8, 1.2)), # Data Augmentation\n",
    "                                    v2.RandomResizedCrop((224, 224), scale=(0.35, 1))\n",
    "\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([ \n",
    "                                    v2.ToImage(),  # Convert numpy array to tensor\n",
    "                                    v2.Normalize(mean=[mu], std=[std_mg]),  # Use mean and std from preprocessing notebook\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5124ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "returning-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"./rsna_heart_detection.csv\"\n",
    "patients_path = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/train_subjects_det.npy\"\n",
    "train_root = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/train/\"\n",
    "dataset = CardiacDataset(labels_path, patients_path, train_root, train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224]) torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/c_0d36ss7sq2scgssjrhgbrc0000gn/T/ipykernel_29489/2973922368.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "img, bbox = dataset[0]\n",
    "print(img.shape, bbox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8606666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b625b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5eef1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/c_0d36ss7sq2scgssjrhgbrc0000gn/T/ipykernel_29489/2973922368.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, bbox = dataset[0]\n",
    "# img=img.unsqueeze(0)\n",
    "# img=img.transpose(0,1)\n",
    "bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "maritime-contemporary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/c_0d36ss7sq2scgssjrhgbrc0000gn/T/ipykernel_29489/2973922368.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (224,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m bbox \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m fig, axis \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m rect \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mRectangle((bbox[\u001b[38;5;241m0\u001b[39m], bbox[\u001b[38;5;241m1\u001b[39m]), bbox[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mbbox[\u001b[38;5;241m0\u001b[39m], bbox[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m-\u001b[39mbbox[\u001b[38;5;241m1\u001b[39m], edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m axis\u001b[38;5;241m.\u001b[39madd_patch(rect)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5976\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5976\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5977\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5979\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/image.py:685\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    684\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/image.py:653\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    651\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (224,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, bbox = dataset[0]\n",
    "# img=img.unsqueeze(0)\n",
    "img=img.squeeze(0)\n",
    "bbox = bbox.squeeze(0)\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "axis.imshow(img[0], cmap=\"bone\")\n",
    "rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], edgecolor=\"r\", facecolor=\"none\")\n",
    "axis.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automatic-review",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/c_0d36ss7sq2scgssjrhgbrc0000gn/T/ipykernel_29489/2973922368.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (224,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m fig, axis \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m spot1 \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mRectangle((label[\u001b[38;5;241m0\u001b[39m], label[\u001b[38;5;241m1\u001b[39m]), label[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mlabel[\u001b[38;5;241m0\u001b[39m], label[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m-\u001b[39mlabel[\u001b[38;5;241m1\u001b[39m], edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m axis\u001b[38;5;241m.\u001b[39madd_patch(spot1)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5976\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5976\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5977\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5979\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/image.py:685\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    684\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/matplotlib/image.py:653\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    651\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (224,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = dataset[17]\n",
    "# img=img.unsqueeze(0)\n",
    "img=img.squeeze(0)\n",
    "label = label.squeeze(0)\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "axis.imshow(img[0], cmap=\"bone\")\n",
    "spot1 = patches.Rectangle((label[0], label[1]), label[2]-label[0], label[3]-label[1], edgecolor='r', facecolor='none')\n",
    "axis.add_patch(spot1)\n",
    "\n",
    "axis.set_title(\"X-RAY with BBOX around heart\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-crime",
   "metadata": {},
   "source": [
    "Awesome! Now we can move to training the heart detection network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenvAImed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
