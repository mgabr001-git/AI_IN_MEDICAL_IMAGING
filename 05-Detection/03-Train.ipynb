{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "saved-cartoon",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We are ready to train the Cardiac Detection Model now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-float",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "\n",
    "* torch and torchvision for model and dataloader creation\n",
    "* pytorch lightning for efficient and easy training implementation\n",
    "* ModelCheckpoint and TensorboardLogger for checkpoint saving and logging\n",
    "* numpy data loading\n",
    "* cv2 for drawing rectangles on images\n",
    "* imgaug for augmentation pipeline\n",
    "* Our CardiacDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cosmetic-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "\n",
    "from torchvision.tv_tensors import BoundingBoxes\n",
    "from torchvision.transforms import v2\n",
    "from dataset import CardiacDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-switch",
   "metadata": {},
   "source": [
    "We create the dataset objects and the augmentation parameters to specify the augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551d70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.49\n",
    "std_mg = 0.082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72659238",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root_path = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/train/\"\n",
    "train_subjects = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/train_subjects_det.npy\"\n",
    "val_root_path = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/val/\"\n",
    "val_subjects = \"../Data/rsna-pneumonia-detection-challenge/Processed-Heart-Detection/val_subjects_det.npy\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1154af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "advanced-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_root_path = \"Processed-Heart-Detection/train/\"\n",
    "# train_subjects = \"train_subjects.npy\"\n",
    "# val_root_path = \"Processed-Heart-Detection/val/\"\n",
    "# val_subjects = \"val_subjects.npy\"\n",
    "\n",
    "# train_transforms = iaa.Sequential([\n",
    "#                                 iaa.GammaContrast(),\n",
    "#                                 iaa.Affine(\n",
    "#                                     scale=(0.8, 1.2),\n",
    "#                                     rotate=(-10, 10),\n",
    "#                                     translate_px=(-10, 10)\n",
    "#                                 )\n",
    "#                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b5a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9757335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "                                    # v2.ToImage(),  # Convert numpy array to tensor\n",
    "                                    v2.Normalize(mean=[mu], std=[std_mg]), # (0.49, 0.248),  # Use mean and std from preprocessing notebook\n",
    "                                    v2.RandomAutocontrast(),\n",
    "                                    v2.RandomAffine(degrees=(-10, 10), translate=(0, 0.05), scale=(0.8, 1.2)), # Data Augmentation\n",
    "                                    v2.RandomResizedCrop((224, 224), scale=(0.35, 1))\n",
    "\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([ \n",
    "                                    # v2.ToImage(),  # Convert numpy array to tensor\n",
    "                                    v2.Normalize(mean=[mu], std=[std_mg]),  # Use mean and std from preprocessing notebook\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079e6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d205dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considered-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CardiacDataset(\"./rsna_heart_detection.csv\", train_subjects, train_root_path, train_transforms)\n",
    "val_dataset = CardiacDataset(\"./rsna_heart_detection.csv\", val_subjects, val_root_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d282bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224]) torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgabr001/Documents/UDEMY/AI-IN-MEDICAL-MATERIALS/05-Detection/dataset.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "img, bbox = train_dataset[0]\n",
    "print(img.shape, bbox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "insured-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solved-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 train images and 96 val images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CardiacDataset(\n",
    "    \"./rsna_heart_detection.csv\",\n",
    "     train_subjects,\n",
    "     train_root_path,\n",
    "     augs = train_transforms)\n",
    "\n",
    "val_dataset = CardiacDataset(\n",
    "    \"./rsna_heart_detection.csv\",\n",
    "     val_subjects,\n",
    "     val_root_path,\n",
    "     augs=None)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-beast",
   "metadata": {},
   "source": [
    "Adapt batch size and num_workers according to your computing hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add72a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd5ebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgabr001/Documents/UDEMY/AI-IN-MEDICAL-MATERIALS/05-Detection/dataset.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.8254,  1.1521,  0.1458,  ..., -1.0988, -0.2859,  0.1934],\n",
       "          [-1.1464, -3.0578, -4.0627,  ..., -5.2581, -5.1628, -4.6842],\n",
       "          [-4.1580, -5.0191, -5.4494,  ..., -5.4974, -5.4494, -5.5450],\n",
       "          ...,\n",
       "          [-5.3061, -5.3061, -5.3061,  ..., -5.4017, -5.4017, -5.4974],\n",
       "          [-5.3061, -5.3061, -5.3061,  ..., -5.4017, -5.4017, -5.4494],\n",
       "          [-5.3061, -5.3061, -5.3061,  ..., -5.4017, -5.4017, -5.4494]]]),\n",
       " tensor([ 61,  59, 169, 176]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8#TODO\n",
    "num_workers = 0# TODO\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-forum",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-processor",
   "metadata": {},
   "source": [
    "We use the same architecture as we used in the classifcation task with some small adaptations:\n",
    "\n",
    "1. 4 outputs: Instead of predicting a binary label we need to estimate the location of the heart (xmin, ymin, xmax, ymax).\n",
    "2. Loss function: Instead of using a cross entropy loss, we are going to use the L2 loss (Mean Squared Error), as we are dealing with continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CardiacDetectionModel(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.model = torchvision.models.resnet18(pretrained=True)\n",
    "#         self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.model.fc = torch.nn.Linear(in_features=512 ,out_features=4)\n",
    "        \n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "#         self.loss_fn = torch.nn.MSELoss()\n",
    "        \n",
    "#     def forward(self, data):\n",
    "#         return self.model(data)\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x_ray, label = batch\n",
    "#         label = label.float()\n",
    "#         pred = self(x_ray)\n",
    "#         loss = self.loss_fn(pred, label)\n",
    "        \n",
    "#         self.log(\"Train Loss\", loss)\n",
    "        \n",
    "#         if batch_idx % 50 == 0:\n",
    "#             self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Train\")\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x_ray, label = batch\n",
    "#         label = label.float()\n",
    "#         pred = self(x_ray)\n",
    "#         loss = self.loss_fn(pred, label)\n",
    "        \n",
    "#         self.log(\"Val Loss\", loss)\n",
    "        \n",
    "#         if batch_idx % 50 == 0:\n",
    "#             self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Val\")\n",
    "#         return loss\n",
    "    \n",
    "#     def log_images(self, x_ray, pred, label, name):\n",
    "#         results = []\n",
    "        \n",
    "#         for i in range(4):\n",
    "#             coords_labels = label[i]\n",
    "#             coords_pred = pred[i]\n",
    "            \n",
    "#             img = ((x_ray[i] * 0.252)+0.494).numpy()[0]\n",
    "            \n",
    "#             x0, y0 = coords_labels[0].int().item(), coords_labels[1].int().item()\n",
    "#             x1, y1 = coords_labels[2].int().item(), coords_labels[3].int().item()\n",
    "#             img = cv2.rectangle(img, (x0, y0), (x1, y1), (0, 0, 0), 2)\n",
    "            \n",
    "#             x0, y0 = coords_pred[0].int().item(), coords_pred[1].int().item()\n",
    "#             x1, y1 = coords_pred[2].int().item(), coords_pred[3].int().item()\n",
    "#             img = cv2.rectangle(img, (x0, y0), (x1, y1), (1, 1, 1), 2)\n",
    "            \n",
    "#             results.append(torch.tensor(img).unsqueeze(0))\n",
    "        \n",
    "#         grid = torchvision.utils.make_grid(results, 2)\n",
    "#         self.logger.experiment.add_image(name, grid, self.global_step)\n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "#         #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "#         return [self.optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neutral-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDetectionModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Change conv1 from 3 to 1 input channels\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Change out_feature of the last fully connected layer (called fc in resnet18) from 1000 to 4\n",
    "        self.model.fc = torch.nn.Linear(in_features=512, out_features=4)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_ray, label = batch\n",
    "        label = label.float()  # Convert label to float (just needed for loss computation)\n",
    "        pred = self(x_ray)\n",
    "        loss = self.loss_fn(pred, label)  # Compute the loss\n",
    "        \n",
    "        # Log loss\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Train\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Same steps as in the training_step\n",
    "        x_ray, label = batch\n",
    "        label = label\n",
    "\n",
    "        label = label.float()  # Convert label to float (just needed for loss computation)\n",
    "        pred = self(x_ray)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Val\")\n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, x_ray, pred, label, name):\n",
    "        results = []\n",
    "        \n",
    "        # Here we create a grid consisting of 4 predictions\n",
    "        for i in range(4):\n",
    "            coords_labels = label[i]\n",
    "            coords_pred = pred[i]\n",
    "            img = ((x_ray[i] * std_mg) + mu).numpy()[0]\n",
    "            \n",
    "            # Extract the coordinates from the label\n",
    "            x0, y0 = coords_labels[0].int().item(), coords_labels[1].int().item()\n",
    "            x1, y1 = coords_labels[2].int().item(), coords_labels[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (0, 0, 0), 2)\n",
    "            \n",
    "            # Extract the coordinates from the prediction           \n",
    "            x0, y0 = coords_pred[0].int().item(), coords_pred[1].int().item()\n",
    "            x1, y1 = coords_pred[2].int().item(), coords_pred[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (1, 1, 1), 2)\n",
    "            \n",
    "            \n",
    "            results.append(torch.tensor(img).unsqueeze(0))\n",
    "        grid = torchvision.utils.make_grid(results, 2)\n",
    "        self.logger.experiment.add_image(f\"{name} Prediction vs Label\", grid, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "second-stuart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create the model object\n",
    "model = CardiacDetectionModel()  # Instanciate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee216eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "\n",
    "# random_input = torch.randn(1, 1, 224, 224)\n",
    "# print(random_input.shape)\n",
    "# output = model(random_input)\n",
    "# output.shape\n",
    "# # assert output.shape == torch.Size([1, 1, 224, 224])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becoming-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    dirpath='./weights',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-cylinder",
   "metadata": {},
   "source": [
    "Train for at least 50 epochs to get a decent result.\n",
    "100 epochs lead to great results.\n",
    "\n",
    "You can train this on a CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "extended-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "# gpus = 1 #TODO\n",
    "trainer = pl.Trainer( accelerator='auto', logger=TensorBoardLogger(\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback, max_epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "massive-bunch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/mgabr001/Documents/UDEMY/AI-IN-MEDICAL-MATERIALS/05-Detection/weights exists and is not empty.\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model   | ResNet  | 11.2 M | train\n",
      "1 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.689    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5d0df1fe7540d2bd4955b232a949f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/mgabr001/Documents/UDEMY/AI-IN-MEDICAL-MATERIALS/05-Detection/dataset.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n",
      "/opt/anaconda3/envs/pytorchenvAImed/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6cd739c8ff4a5aaa49766e38bd7c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7de2bb650f4a188cefe5feb871bd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ba64d5a96042518b5e8c04ea57edb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1588d04fcd44f6a4d65d41c1aec8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9529ca7fd6d44dcaa73d0b2981a77265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62ed5bc59074249b78bfb5cba65bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train the detection model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-designer",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "listed-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-royalty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.load_from_checkpoint(\"weight.ckpt\")\n",
    "model.eval();\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-contemporary",
   "metadata": {},
   "source": [
    "Compute prediction for all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-witness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in val_dataset:\n",
    "        data = data.to(device).float().unsqueeze(0)\n",
    "        pred = model(data)[0].cpu()\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "preds=torch.stack(preds)\n",
    "labels=torch.stack(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-cleanup",
   "metadata": {},
   "source": [
    "Compute mean deviation between prediction and labels for each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-construction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs(preds-labels).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-video",
   "metadata": {},
   "source": [
    "Example prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-shield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IDX = 60  # Feel free to inspect all validation samples by changing the index\n",
    "img, label = val_dataset[IDX]\n",
    "current_pred = preds[IDX]\n",
    "\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "axis.imshow(img[0], cmap=\"bone\")\n",
    "heart = patches.Rectangle((current_pred[0], current_pred[1]), current_pred[2]-current_pred[0],\n",
    "                          current_pred[3]-current_pred[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "axis.add_patch(heart)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-snapshot",
   "metadata": {},
   "source": [
    "Awesome, looks like we got a working heart detection!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenvAImed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
